using BEPUutilities2.Collections;
using BEPUutilities2.Memory;
using SolverPrototype.Collidables;
using System.Numerics;
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;
using System;
using SolverPrototype.Constraints;
using System.Diagnostics;
using System.Threading;
using BEPUutilities2;

namespace SolverPrototype.CollisionDetection
{
    /*
     * The narrow phase operates on overlaps generated by the broad phase. 
     * Its job is to compute contact manifolds for overlapping collidables and to manage the constraints produced by those manifolds. 
     * 
     * The scheduling of collision detection jobs is conceptually asynchronous. There is no guarantee that a broad phase overlap provided to the narrow phase
     * will result in an immediate calculation of the manifold. This is useful for batching together many collidable pairs of the same type for simultaneous SIMD-friendly execution.
     * (Not all pairs are ideal fits for wide SIMD, but many common and simple ones are.)
     * 
     * The interface to the broad phase makes no guarantees about the nature of this batching. The narrow phase could immediately execute, or it could batch up Vector<float>.Count,
     * or maybe 32 in a row, or it could wait until all overlaps have been submitted before actually beginning work.
     * 
     * This deferred execution requires that the pending work be stored somehow. This is complicated by the fact that there are a variety of different top level pairs that handle
     * incoming contact manifold data and the resulting constraints in different ways. There are two main distinctions:
     * 1) Continuous collision detection mode. For the purposes of the narrow phase, each collidable can be thought of as discrete, inner sphere, substepping, or inner sphere + substepping.
     * -Discrete pairs take the result of the underlying manifold and directly manipulate regular contact constraints. 
     * -Inner sphere pairs, with sufficient relative linear velocity, can create one or two additional sphere-convex pairs per convex pair.
     * -Substepping pairs potentially generate a bunch of child pairs, depending on the collidable velocities, and then choose from the resulting manifolds. 
     * Once the best manifold is selected, constraint management is similar to the discrete case.
     * -Inner sphere + substepping pairs just do both of the above.
     * 2) Individual versus compound types. Compound pairs will tend to create child convex pairs and wait for their completion. This ensures the greatest number of simultaneous
     * SIMD-friendly manifold calculations. For example, four compound-compound pairs could result in 60 sphere-capsule subpairs which can then all be executed in a SIMD fashion.
     * 
     * These two build on each other- a compound-compound pair with inner sphere enabled will want to generate both the inner sphere pairs and the regular pairs simultaneously to avoid 
     * traversing any acceleration structures multiple times.
     * 
     * Note that its possible for the evaluation of a pair to generate more pairs. This is most easily seen in compound pairs or substep pairs, but we do permit less obvious cases.
     * For example, a potential optimization for substepping is only do as many substeps as are needed to find the first manifold with approaching contacts (or some other heuristic).
     * In order for such an optimization to be used, we must be willing to spawn more pairs if the first set of substeps we did didn't find any heuristically accepted manifolds.
     * In the limit, that would mean doing one substep at a time. (In practice, we'd probably just try to fill up the remainder of a SIMD batch.)
     * 
     * Another example: imagine a high-complexity convex-convex test that has highly divergent execution, but with smaller pieces which are not as divergent.
     * SIMD operations don't map well to divergent execution, so if the individual jobs are large enough, it could be worth it to spawn new pairs for the nondivergent pieces.
     * Most convexes aren't complicated enough to warrant this (often it's faster to simply execute all paths), but it may be relevant in the convex hull versus convex hull case.
     * 
     * In any case where more pairs are generated, evaluating just the current set of pairs is insufficient to guarantee completion. Instead, execution can be thought of like traversing a graph.
     * Each work-creating pair may create an entry on the execution stack if its 'execution threshold' is reached (the arbitrary size which, when reached, results in the execution of the 
     * stored pairs). When no jobs remain on the stack, take any available stored pair set and try to execute it- even if it hasn't yet reached its execution threshold. In this situation,
     * without further action it won't ever fill up, so there's no reason to wait. That execution may then spawn more work, which could create an element on the execution stack, and so on. 
     * Ideally, job sets are consumed in order of their probability of creating new work. That maximizes the number of SIMD-friendly executions.
     * 
     * In practice, there are two phases. The first phase takes in the broad phase-generated top level pairs. At this stage, we do not need to resort to executing incomplete bundles. 
     * Instead, we just continue to work on the top level pairs until none remain. The second phase kicks in here. Since no further top-level work is being generated, we start trying to 
     * flush all the remaining pairs, even if they are not at the execution threshold, as in the above traverse-and-reset approach.
     * 
     * All of the above works within the context of a single thread. There may be many threads in flight, but each one is guaranteed to be handling different top level pairs.
     * That means all of the pair storage is thread local and requires no synchronization. It is also mostly ephemeral- once the thread finishes, only a small amount of information needs
     * to be persisted to globally accessed memory. (Overlap->ConstraintHandle is one common piece of data, but some pairs may also persist other data like separating axes for early outs.
     * Such extra data is fairly rare, since it implies divergence in execution- which is something you don't want in a SIMD-friendly implementation. Likely only in things like hull-hull.)
     * 
     * Every narrow phase pair is responsible for managing the constraints that its computed manifolds require. 
     * This requires the ability to look up existing overlap->constraint relationships for three reasons:
     * 1) Any existing constraint, if it has the same number of contacts as the new manifold, should have its contact data updated.
     * 2) Any accumulated impulse from the previous frame's contact solve should be distributed over the new set of contacts for warm starting this frame's solve.
     * 3) Any change in contact count should result in the removal of the previous constraint (if present) and the addition of the new constraint (if above zero contacts).
     * This mapping is stored in a single dictionary. The previous frame's mapping is treated as read-only during the new frame's narrow phase execution, 
     * //so no synchronization is required to read it. The current frame updates pointerse in the dictionary and collects deferred adds on each worker thread for later flushing.
     * 
     * Constraints associated with 'stale' overlaps (those which were not updated during the current frame) are removed in a postpass.
     * 
     */




    public abstract class NarrowPhase
    {
        public BufferPool Pool;
        public Bodies Bodies;
        public Solver Solver;
        public ConstraintRemover ConstraintRemover;
        public FreshnessChecker FreshnessChecker;
        //TODO: It is possible that some types will benefit from per-overlap data, like separating axes. For those, we should have type-dedicated overlap dictionaries.
        //The majority of type pairs, however, only require a constraint handle.
        public PairCache PairCache;
        //TODO: Need to check codegen on this. In some cases when everything involved is a reference type, the JIT won't devirtualize- or at least, that was how it used to be.
        public abstract void HandleOverlap(int workerIndex, CollidableReference a, CollidableReference b);

        public static TNarrowPhase Create<TNarrowPhase>(Bodies bodies, Solver solver, ConstraintConnectivityGraph constraintGraph, BufferPool pool,
            int minimumMappingSize = 2048, int minimumPendingSize = 128, int minimumPerTypeCapacity = 128)
            where TNarrowPhase : NarrowPhase, new()
        {
            var narrowPhase = new TNarrowPhase
            {
                Pool = pool,
                Bodies = bodies,
                Solver = solver,
                PairCache = new PairCache(pool, minimumMappingSize, minimumPendingSize, minimumPerTypeCapacity),
                ConstraintRemover = new ConstraintRemover(pool, bodies, solver, constraintGraph, minimumRemovalCapacity: minimumPendingSize),
            };
            narrowPhase.FreshnessChecker = new FreshnessChecker(narrowPhase);

            return narrowPhase;
        }

        public NarrowPhase()
        {
            flushWorkerLoop = FlushWorkerLoop;
        }

        public void Prepare(IThreadDispatcher threadDispatcher = null)
        {
            PairCache.Prepare(threadDispatcher);
            ConstraintRemover.Prepare(threadDispatcher);
        }

        protected abstract void OnFlush(IThreadDispatcher threadDispatcher = null);

        int flushJobIndex;
        QuickList<NarrowPhaseFlushJob, Buffer<NarrowPhaseFlushJob>> flushJobList;
        Action<int> flushWorkerLoop;
        void FlushWorkerLoop(int workerIndex)
        {
            int jobIndex;
            while ((jobIndex = Interlocked.Increment(ref flushJobIndex)) < flushJobList.Count)
            {
                ExecuteFlushJob(ref flushJobList[jobIndex]);
            }
        }

        void ExecuteFlushJob(ref NarrowPhaseFlushJob job)
        {
            switch (job.Type)
            {
                case NarrowPhaseFlushJobType.RemoveConstraintsFromBodyLists:
                    ConstraintRemover.RemoveConstraintsFromBodyLists();
                    break;
                case NarrowPhaseFlushJobType.RemoveConstraintFromTypeBatch:
                    ConstraintRemover.RemoveConstraintsFromTypeBatch(job.Index);
                    break;
                case NarrowPhaseFlushJobType.RemoveBodyHandlesFromBatches:
                    ConstraintRemover.RemoveBodyHandlesFromBatches();
                    break;
                case NarrowPhaseFlushJobType.ReturnConstraintHandlesToPool:
                    ConstraintRemover.ReturnConstraintHandlesToPool();
                    break;
                case NarrowPhaseFlushJobType.FlushPairCacheChanges:
                    PairCache.FlushMappingChanges();
                    break;
            }

        }

        public void Flush(IThreadDispatcher threadDispatcher = null)
        {
            FreshnessChecker.CheckFreshness(threadDispatcher);

            //Given the sizes involved, a fixed guess of 128 should be just fine for essentially any simulation. Overkill, but not in a concerning way.
            //Temporarily allocating 1KB of memory isn't a big deal, and we will only touch the necessary subset of it anyway.
            //(There are pathological cases where resizes are still possible, but the constraint remover handles them by not adding unsafely.)
            QuickList<NarrowPhaseFlushJob, Buffer<NarrowPhaseFlushJob>>.Create(Pool.SpecializeFor<NarrowPhaseFlushJob>(), 128, out flushJobList);
            PairCache.PrepareFlushJobs(ref flushJobList);
            ConstraintRemover.CreateFlushJobs(ref flushJobList);

            if (threadDispatcher == null)
            {
                for (int i = 0; i < flushJobList.Count; ++i)
                {
                    ExecuteFlushJob(ref flushJobList[i]);
                }
            }
            else
            {
                flushJobIndex = -1;
                threadDispatcher.DispatchWorkers(flushWorkerLoop);
            }
            flushJobList.Dispose(Pool.SpecializeFor<NarrowPhaseFlushJob>());

            PairCache.Postflush();
            ConstraintRemover.Postflush();

            OnFlush();
        }

        public void Dispose()
        {
            PairCache.Dispose();
            OnDispose();
        }

        protected abstract void OnDispose(); 

        //TODO: Configurable memory usage. It automatically adapts based on last frame state, but it's nice to be able to specify minimums when more information is known.

    }


    /// <summary>
    /// Turns broad phase overlaps into contact manifolds and uses them to manage constraints in the solver.
    /// </summary>
    /// <typeparam name="TCallbacks">Type of the callbacks to use.</typeparam>
    public partial class NarrowPhase<TCallbacks> : NarrowPhase where TCallbacks : struct, INarrowPhaseCallbacks
    {
        public TCallbacks Callbacks;


        protected override void OnFlush(IThreadDispatcher threadDispatcher = null)
        {
            Callbacks.Flush(threadDispatcher);
        }

      

        protected override void OnDispose()
        {
            Callbacks.Dispose();
        }
        

        /// <summary>
        /// Continuations accumulated for a worker.
        /// </summary>
        public struct ContinuationCache
        {
            public BufferPool Pool;
            QuickList<Substeps, Buffer<Substeps>> substeps;
            int initialCapacityPerType;
            public ContinuationCache(BufferPool pool, int initialCapacityPerType = 32)
            {
                Pool = pool;
                this.initialCapacityPerType = initialCapacityPerType;
                substeps = new QuickList<Substeps, Buffer<Substeps>>();
            }

            [MethodImpl(MethodImplOptions.AggressiveInlining)]
            private ref T Add<T>(ref QuickList<T, Buffer<T>> list)
            {
                var index = list.Count;
                var newCount = list.Count + 1;
                var minimumCount = Math.Max(newCount, initialCapacityPerType);
                var pool = Pool.SpecializeFor<T>();
                if (list.Span.Allocated)
                    list.EnsureCapacity(minimumCount, pool);
                else
                    QuickList<T, Buffer<T>>.Create(pool, minimumCount, out list);
                list.Count = newCount;
                return ref list[index];
            }

            public void AddSubstep()
            {
                ref var slot = ref Add(ref substeps);
            }
        }

        Buffer<ContinuationCache> continuationCaches;

        unsafe struct LinearAndSubsteps
        {
            //Note that we can't immediately dispatch constraint generation when only one of the substeps or inner sphere tests completes.
            //We have to wait for them all. So, we have to store the intermediate results.
            public Substeps Substeps;
            public Linear Linear;

            public bool Notify()
            {
                //Note that we just use the substep's counter for both it and the linear portion.
                if (Substeps.CompletedSubsteps == Substeps.SubstepCount + 1)
                {
                    //There are only a total of Substeps.SubstepCount + 2 jobs available. All manifolds are now filled.
                    return true;
                }
                ++Substeps.CompletedSubsteps;
                return false;
            }

            public void Trigger(ContactManifold* outManifold)
            {
                Debug.Assert(Substeps.CompletedSubsteps == Substeps.SubstepCount + 1);
                //Attempt to create a single manifold which best represents all submanifolds.
                ContactManifold substepManifold;
                Substeps.Trigger(&substepManifold);
                //Note that we assume that this struct is pinned. That is safe; the narrow phase structures are all stored in pinned or unmanaged memory buffers.
                Linear.CombineManifolds(&substepManifold, (ContactManifold*)Unsafe.AsPointer(ref Linear.Manifold), outManifold);
            }
        }

        unsafe struct LinearNoSubsteps
        {
            public ContactManifold Manifold;
            public ContactManifold Linear;
            public int SubmanifoldsCompleted;

            public bool Notify()
            {
                return ++SubmanifoldsCompleted == 3;
            }


        }

        /// <summary>
        /// Stores the inner sphere manifolds associated with a linear-including pair.
        /// </summary>
        unsafe struct Linear
        {
            //TODO: This is a situation where a special case would be beneficial.
            //These inner sphere reports can never contribute more than one contact (so this will only hold up to two contacts),
            //but these manifolds preallocate enough room for a full 4 nonconvex contacts.
            public ContactManifold Manifold;


            public static void CombineManifolds(ContactManifold* mainManifold, ContactManifold* linearManifold, ContactManifold* outManifold)
            {
                var linearCount = linearManifold->ContactCount;
                if (linearCount > 0)
                {
                    Debug.Assert(linearCount <= 2, "Inner sphere derived contacts should only ever contribute one contact per involved body.");

                    var mainCount = mainManifold->ContactCount;
                    if (mainCount > 0)
                    {
                        var totalCount = mainCount + linearCount;
                        var contactsToPotentiallyReplaceCount = totalCount - 4;
                        if (contactsToPotentiallyReplaceCount > 0)
                        {
                            //There are more contacts than slots. A subset must be prioritized.
                            //We want the deepest set of contacts from all available manifolds. 
                            //(This isn't necessarily an ideal heuristic- consider a bunch of deep  contacts all in the same place,
                            //causing the removal of a distant but less redundant contact. In practice, though, it works okay.)

                            //To find the deepest contacts, simply sort both sets and pop the minimums.
                            //Rather than shuffling the contact manifold memory around, just sort indices.

                            //TODO: This entire hardcoded sort is a bit gross and silly. You could do better.
                            //TODO: It's not clear that this is actually even useful. It may be that just picking the deepest of the linear contacts and filling any open space
                            //is the best and simplest option in the end.
                            var linearIndices = stackalloc int[linearCount];
                            var mainIndices = stackalloc int[mainCount];
                            if (linearCount == 2)
                            {
                                if (linearManifold->Depth0 < linearManifold->Depth1)
                                {
                                    linearIndices[0] = 0;
                                    linearIndices[1] = 1;
                                }
                                else
                                {
                                    linearIndices[0] = 0;
                                    linearIndices[1] = 1;
                                }
                            }
                            else
                            {
                                linearIndices[0] = 0;
                            }

                            for (int i = 0; i < mainCount; ++i)
                                mainIndices[i] = i;

                            outManifold->SetConvexityAndCount(totalCount, false);
                            var mainDepths = &mainManifold->Depth0;
                            for (int i = 1; i <= mainCount; ++i)
                            {
                                var originalIndex = mainIndices[i];
                                var depth = mainDepths[originalIndex];
                                int compareIndex;
                                for (compareIndex = i - 1; compareIndex >= 0; --compareIndex)
                                {
                                    var compareDepth = mainDepths[compareIndex];
                                    if (compareDepth < depth)
                                    {
                                        //Move the element up one slot.
                                        var upperSlotIndex = compareIndex + 1;
                                        mainIndices[upperSlotIndex] = mainIndices[compareIndex];
                                    }
                                    else
                                        break;
                                }
                                var targetIndex = compareIndex + 1;
                                if (targetIndex != i)
                                {
                                    //Move the original index down.
                                    mainIndices[targetIndex] = originalIndex;
                                }
                            }

                            var outIndex = 0;
                            var mainIndex = 0;
                            var linearIndex = 0;
                            var outOffsets = &outManifold->Offset0;
                            var outDepths = &outManifold->Depth0;
                            var outBases = &outManifold->SurfaceBasis0;
                            var outIds = &outManifold->FeatureId0;
                            var linearOffsets = &linearManifold->Offset0;
                            var linearDepths = &linearManifold->Depth0;
                            var linearBases = &linearManifold->SurfaceBasis0;
                            var linearIds = &linearManifold->FeatureId0;
                            var mainOffsets = &mainManifold->Offset0;
                            var mainIds = &mainManifold->FeatureId0;
                            if (mainManifold->Convex)
                            {
                                //While the linear and output manifolds are nonconvex, the main one is convex.
                                while (outIndex < 4)
                                {
                                    if (linearIndex == linearCount ||
                                        (mainIndex < mainCount &&
                                        mainDepths[mainIndex] > linearDepths[linearIndex]))
                                    {
                                        outOffsets[outIndex] = mainOffsets[mainIndex];
                                        outDepths[outIndex] = mainDepths[mainIndex];
                                        outBases[outIndex] = mainManifold->ConvexSurfaceBasis;
                                        outIds[outIndex] = mainIds[mainIndex];
                                        ++mainIndex;
                                    }
                                    else
                                    {
                                        outOffsets[outIndex] = linearOffsets[linearIndex];
                                        outDepths[outIndex] = linearDepths[linearIndex];
                                        outBases[outIndex] = linearBases[linearIndex];
                                        outIds[outIndex] = linearIds[linearIndex];
                                        ++linearIndex;
                                    }
                                }
                            }
                            else
                            {
                                //Both manifolds are nonconvex.
                                var mainBases = &mainManifold->SurfaceBasis0;
                                while (outIndex < 4)
                                {
                                    if (linearIndex == linearCount ||
                                        (mainIndex < mainCount &&
                                        mainDepths[mainIndex] > linearDepths[linearIndex]))
                                    {
                                        outOffsets[outIndex] = mainOffsets[mainIndex];
                                        outDepths[outIndex] = mainDepths[mainIndex];
                                        outBases[outIndex] = mainBases[mainIndex];
                                        outIds[outIndex] = mainIds[mainIndex];
                                        ++mainIndex;
                                    }
                                    else
                                    {
                                        outOffsets[outIndex] = linearOffsets[linearIndex];
                                        outDepths[outIndex] = linearDepths[linearIndex];
                                        outBases[outIndex] = linearBases[linearIndex];
                                        outIds[outIndex] = linearIds[linearIndex];
                                        ++linearIndex;
                                    }
                                    ++outIndex;
                                }
                            }
                        }
                        else
                        {
                            //There is sufficient room in the manifold to include all the new contacts, so there is no need for prioritization.
                            outManifold->SetConvexityAndCount(totalCount, false);
                            //Add all existing contacts. Note that the use of inner sphere contacts forces the manifold to be nonconvex unconditionally.
                            //While there are cases in which the normals could actually be planar, we don't spend the time figuring that out-
                            //this state will be extremely brief regardless, and there isn't much value in trying to tease out convexity for one or two frames.
                            var outOffsets = &outManifold->Offset0;
                            var outDepths = &outManifold->Depth0;
                            var outBases = &outManifold->SurfaceBasis0;
                            var outIds = &outManifold->FeatureId0;
                            var mainOffsets = &mainManifold->Offset0;
                            var mainDepths = &mainManifold->Depth0;
                            var mainIds = &mainManifold->FeatureId0;
                            if (mainManifold->Convex)
                            {
                                for (int i = 0; i < mainCount; ++i)
                                {
                                    outOffsets[i] = mainOffsets[i];
                                    outDepths[i] = mainDepths[i];
                                    outBases[i] = mainManifold->ConvexSurfaceBasis;
                                    outIds[i] = mainIds[i];
                                }
                            }
                            else
                            {
                                var mainBases = &mainManifold->SurfaceBasis0;
                                for (int i = 0; i < mainCount; ++i)
                                {
                                    outOffsets[i] = mainOffsets[i];
                                    outDepths[i] = mainDepths[i];
                                    outBases[i] = mainBases[i];
                                    outIds[i] = mainIds[i];
                                }
                            }
                            //Now add the linear contacts. Both manifolds are known to be nonconvex. 
                            var linearOffsets = &linearManifold->Offset0;
                            var linearDepths = &linearManifold->Depth0;
                            var linearBases = &linearManifold->SurfaceBasis0;
                            var linearIds = &linearManifold->FeatureId0;
                            for (int linearIndex = 0; linearIndex < linearCount; ++linearIndex)
                            {
                                var outIndex = mainCount + linearIndex;
                                outOffsets[outIndex] = linearOffsets[linearIndex];
                                outDepths[outIndex] = linearDepths[linearIndex];
                                outBases[outIndex] = linearBases[linearIndex];
                                outIds[outIndex] = linearIds[linearIndex];
                            }
                        }
                    }
                    else
                    {
                        outManifold = linearManifold;
                    }
                }
                else
                {
                    outManifold = mainManifold;
                }
            }
        }


        unsafe struct Substeps
        {
            public CollidableReference A;
            public CollidableReference B;
            public int SubstepCount;
            public int CompletedSubsteps;
            public int NextSubstepStart;
            public Buffer<ContactManifold> Manifolds;
            /// <summary>
            /// Change in the offset from A's position to B's position from the current frame's pose at t=0 to the predicted pose at t=1.
            /// Equal to the integrated relative linear velocity.
            /// </summary>
            public Vector3 RelativeOffsetChange;

            public Substeps(CollidableReference a, CollidableReference b, BufferPool pool, ref Vector3 startOffsetB, ref Vector3 endOffsetB, int substepCount)
            {
                SubstepCount = substepCount;
                pool.SpecializeFor<ContactManifold>().Take(substepCount, out Manifolds);
                NextSubstepStart = 0;
                CompletedSubsteps = 0;
                A = a;
                B = b;
                RelativeOffsetChange = endOffsetB - startOffsetB;
            }

            public ContactManifold* GetManifoldForSubstep(int index)
            {
                return (ContactManifold*)Manifolds.Memory + index;
            }

            public bool Notify()
            {
                //TODO: When using 'first contacts' as a heuristic, we could simply quit on the earliest notification of existing contacts (so long as notifications come in order).
                //This is not as valuable as it might seem due to collisions being handled in SIMD batches, but it may be something worth looking into for other more scalar types like
                //compounds and hulls. Implementing that approach would look something like creating a new substeps continuation once the current batches come back with no contacts.
                //The idea would be that we only try substeps as numerous as necessary to fill the remaining room in relevant pair batches. There's no single such number for compounds, though.
                //Given the relative rarity of substepping invocations, I suspect doing this is pointless.
                if (++CompletedSubsteps == SubstepCount)
                {
                    return true;
                }
                return false;
            }

            public static float GetProgressionForSubstep(int substepIndex, int substepCount)
            {
                return substepIndex / (float)substepCount;
            }

            [MethodImpl(MethodImplOptions.AggressiveInlining)]
            public void Trigger(ContactManifold* substepManifold)
            {
                //Scan the substeps looking for the first substep that contains any contacts.
                //TODO: There are situations involving very high angular velocity where the first contacts are not the best choice. 
                //If this turns out to be a problem in practice, you may want to change the heuristic to prefer *approaching* contacts over merely existing contacts.
                //However, determining whether a contact is approaching requires computing the relative velocity at its position, which isn't free. (Not super expensive, but not free.)
                //For the sake of simplicity, just use 'first contact' for now.

                //TODO: If the first manifold is not full, you could also pull contacts from later substeps. They might help post-collision rotate-through-the-ground type penetration.

                for (int i = 0; i < SubstepCount; ++i)
                {
                    ref var manifold = ref Manifolds[i];
                    var contactCount = manifold.ContactCount;
                    if (contactCount > 0)
                    {
                        *substepManifold = manifold;
                        //Once the best substep is selected, transform the contact positions and depths to be relative to the poses at t=0.
                        //Since the contact position offsets are not rotated, all we have to do is add the offset from t=0 to the current time to each contact position
                        //and modify the penetration depths according to that offset along the normal.
                        var offset = RelativeOffsetChange * GetProgressionForSubstep(i, SubstepCount);
                        var offsets = &substepManifold->Offset0;
                        var depths = &substepManifold->Depth0;
                        //TODO: these two TransformY's could be optimized with knowledge that it's unit length. 
                        //That would be pretty useful generally- I'm not sure we've ever used those functions without it being unit length.
                        //Pretty micro-optimizey, though.
                        if (substepManifold->Convex)
                        {
                            BEPUutilities2.Quaternion.TransformY(1, ref substepManifold->ConvexSurfaceBasis, out var manifoldNormal);
                            var penetrationOffset = Vector3.Dot(offset, manifoldNormal);
                            for (int j = 0; j < contactCount; ++j)
                            {
                                offsets[j] += offset;
                                depths[j] += penetrationOffset;
                            }
                        }
                        else
                        {
                            var bases = &substepManifold->SurfaceBasis0;
                            for (int j = 0; j < contactCount; ++j)
                            {
                                BEPUutilities2.Quaternion.TransformY(1, ref bases[j], out var manifoldNormal);
                                var penetrationOffset = Vector3.Dot(offset, manifoldNormal);
                                offsets[j] += offset;
                                depths[j] += penetrationOffset;
                            }
                        }
                        return;
                    }
                }
                //If there are no contacts, then just return an empty manifold.
                *substepManifold = Manifolds[0];
            }
        }





        public override void HandleOverlap(int workerIndex, CollidableReference a, CollidableReference b)
        {
            if (!Callbacks.AllowContactGeneration(workerIndex, a, b))
                return;
            var staticness = (a.packed >> 31) | ((b.packed & 0x7FFFFFFF) >> 30);
            switch (staticness)
            {
                case 0:
                    {
                        //Both references are bodies.
                        //This is a body. In order to dispatch it properly, we need to know some metadata.
                        //TODO: Once inactive bodies exist, this will need to be updated.
                        ref var aCollidable = ref Bodies.Collidables[Bodies.HandleToIndex[a.Collidable]];
                        ref var bCollidable = ref Bodies.Collidables[Bodies.HandleToIndex[b.Collidable]];
                        //Note that we never create 'unilateral' CCD pairs. That is, if either collidable in a pair enables a CCD feature, we just act like both are using it.
                        //That keeps things a little simpler. Unlike v1, we don't have to worry about the implications of 'motion clamping' here- no need for deeper configuration.
                        var useSubstepping = aCollidable.Continuity.UseSubstepping || bCollidable.Continuity.UseSubstepping;
                        var useInnerSphere = aCollidable.Continuity.UseInnerSphere || bCollidable.Continuity.UseInnerSphere;
                        //Create a continuation for the pair given the CCD state.
                        if (useSubstepping && useInnerSphere)
                        {
                        }
                        else if (useSubstepping)
                        {

                        }
                        else if (useInnerSphere)
                        {

                        }
                        else
                        {
                            //This pair uses no CCD beyond its speculative margin.

                        }

                        //Pull the velocity information for all involved bodies. We will request a number of steps that will cover the motion path.
                        //number of substeps = min(maximum substep count, 1 + floor(estimated displacement / step length)), where
                        //estimated displacement = dt * (length(linear velocity A - linear velocity B) +
                        //                               maximum radius A * (length(angular velocity A) + maximum radius B * length(angular velocity B)) 
                        //Once we have a number of 
                        //We use the minimum step length of each contributing collidable. Treat non-substepping collidables as having a step length of infinity.
                        var stepLengthA = aCollidable.Continuity.UseSubstepping ? aCollidable.Continuity.MaximumStepLength : float.MaxValue;
                        var stepLengthB = bCollidable.Continuity.UseSubstepping ? bCollidable.Continuity.MaximumStepLength : float.MaxValue;
                        float stepLength = stepLengthA < stepLengthB ? stepLengthA : stepLengthB;

                    }
                    break;
                case 1:
                    {
                        //Collidable a is a body, b is a static.
                        //TODO: Once non-body collidables exist, this will need to be updated.
                    }
                    break;
                case 2:
                    {
                        //Collidable a is a static, b is a body.
                        //TODO: Once non-body collidables exist, this will need to be updated.
                    }
                    break;
                case 3:
                    {
                        //Both collidables are statics. This is a bit of a weird situation- under normal conditions, static bodies will belong to the 
                        //'inactive' broad phase tree, and the inactive tree is not tested against itself. The user must have configured this static to be in the active tree to act
                        //as a detector or something along those lines.
                        //TODO: Once non-body collidables exist, this will need to be updated.
                    }
                    break;
            }

        }
    }
}