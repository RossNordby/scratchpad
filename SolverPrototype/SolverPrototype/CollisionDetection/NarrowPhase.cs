using BEPUutilities2.Collections;
using BEPUutilities2.Memory;
using SolverPrototype.Collidables;
using System.Numerics;
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;
using System;
using SolverPrototype.Constraints;
using System.Diagnostics;
using System.Threading;

namespace SolverPrototype.CollisionDetection
{
    /*
     * The narrow phase operates on overlaps generated by the broad phase. 
     * Its job is to compute contact manifolds for overlapping collidables and to manage the constraints produced by those manifolds. 
     * 
     * The scheduling of collision detection jobs is conceptually asynchronous. There is no guarantee that a broad phase overlap provided to the narrow phase
     * will result in an immediate calculation of the manifold. This is useful for batching together many collidable pairs of the same time for simultaneous SIMD-friendly execution.
     * (Not all pairs are ideal fits for SIMD, but many common and simple ones are.)
     * 
     * The interface to the broad phase makes no guarantees about the nature of this batching. The narrow phase could immediately execute, or it could batch up Vector<float>.Count,
     * or maybe 32 in a row, or it could wait until all overlaps have been submitted before actually beginning work.
     * 
     * This deferred execution requires that the pending work be stored somehow. This is complicated by the fact that there are a variety of different top level pairs that handle
     * incoming contact manifold data and the resulting constraints in different ways. There are two main distinctions:
     * 1) Continuous collision detection mode. For the purposes of the narrow phase, each collidable can be thought of as discrete, inner sphere, substepping, or inner sphere + substepping.
     * -Discrete pairs take the result of the underlying manifold and directly manipulate regular contact constraints. 
     * -Inner sphere pairs, with sufficient relative linear velocity, can create one or two additional sphere-convex pairs per convex pair.
     * -Substepping pairs potentially generate a bunch of child pairs, depending on the collidable velocities, and then choose from the resulting manifolds. 
     * Once the best manifold is selected, constraint management is similar to the discrete case.
     * -Inner sphere + substepping pairs just do both of the above.
     * 2) Individual versus compound types. Compound pairs will tend to create child convex pairs and wait for their completion. This ensures the greatest number of simultaneous
     * SIMD-friendly manifold calculations. For example, four compound-compound pairs could result in 60 sphere-capsule subpairs which can then all be executed in a SIMD fashion.
     * 
     * These two build on each other- a compound-compound pair with inner sphere enabled will want to generate both the inner sphere pairs and the regular pairs simultaneously to avoid 
     * traversing any acceleration structures multiple times.
     * 
     * Note that its possible for the evaluation of a pair to generate more pairs. This is most easily seen in compound pairs or substep pairs, but we do permit less obvious cases.
     * For example, a potential optimization for substepping is only do as many substeps as are needed to find the first manifold with approaching contacts (or some other heuristic).
     * In order for such an optimization to be used, we must be willing to spawn more pairs if the first set of substeps we did didn't find any heuristically accepted manifolds.
     * In the limit, that would mean doing one substep at a time. (In practice, we'd probably just try to fill up the remainder of a SIMD batch.)
     * 
     * Another example: imagine a high-complexity convex-convex test that has highly divergent execution, but with smaller pieces which are not as divergent.
     * SIMD operations don't map well to divergent execution, so if the individual jobs are large enough, it could be worth it to spawn new pairs for the nondivergent pieces.
     * Most convexes aren't complicated enough to warrant this (often it's faster to simply execute all paths), but it may be relevant in the convex hull versus convex hull case.
     * 
     * In any case where more pairs are generated, evaluating just the current set of pairs is insufficient to guarantee completion. Instead, execution can be thought of like traversing a graph.
     * Each work-creating pair may create an entry on the execution stack if its 'execution threshold' is reached (the arbitrary size which, when reached, results in the execution of the 
     * stored pairs). When no jobs remain on the stack, take any available stored pair set and try to execute it- even if it hasn't yet reached its execution threshold. In this situation,
     * without further action it won't ever fill up, so there's no reason to wait. That execution may then spawn more work, which could create an element on the execution stack, and so on. 
     * Ideally, job sets are consumed in order of their probability of creating new work. That maximizes the number of SIMD-friendly executions.
     * 
     * In practice, there are two phases. The first phase takes in the broad phase-generated top level pairs. At this stage, we do not need to resort to executing incomplete bundles. 
     * Instead, we just continue to work on the top level pairs until none remain. The second phase kicks in here. Since no further top-level work is being generated, we start trying to 
     * flush all the remaining pairs, even if they are not at the execution threshold, as in the above traverse-and-reset approach.
     * 
     * All of the above works within the context of a single thread. There may be many threads in flight, but each one is guaranteed to be handling different top level pairs.
     * That means all of the pair storage is thread local and requires no synchronization. It is also mostly ephemeral- once the thread finishes, only a small amount of information needs
     * to be persisted to globally accessed memory. (Overlap->ConstraintHandle is one common piece of data, but some pairs may also persist other data like separating axes for early outs.
     * Such extra data is fairly rare, since it implies divergence in execution- which is something you don't want in a SIMD-friendly implementation. Likely only in things like hull-hull.)
     * 
     * Every narrow phase pair is responsible for managing the constraints that its computed manifolds require. 
     * This requires the ability to look up existing overlap->constraint relationships for three reasons:
     * 1) Any existing constraint, if it has the same number of contacts as the new manifold, should have its contact data updated.
     * 2) Any accumulated impulse from the previous frame's contact solve should be distributed over the new set of contacts for warm starting this frame's solve.
     * 3) Any change in contact count should result in the removal of the previous constraint (if present) and the addition of the new constraint (if above zero contacts).
     * This mapping is stored in a single dictionary. The previous frame's mapping is treated as read-only by the new frame, so no synchronization is required to read it. The current frame builds
     * a new dictionary incrementally. It starts from scratch, so only actually-needed overlaps will exist in the new dictionary.
     * 
     * Constraints associated with 'stale' overlaps (those which were not updated during the current frame) are removed in a postpass.
     * 
     */

    //toooo many generics
    using PairCache = QuickDictionary<CollidablePair, PairConstraintReference, Buffer<CollidablePair>, Buffer<PairConstraintReference>, Buffer<int>, CollidablePairComparer>;

    [StructLayout(LayoutKind.Explicit, Size = 8)]
    public struct CollidablePair
    {
        [FieldOffset(0)]
        public CollidableReference A;
        [FieldOffset(4)]
        public CollidableReference B;
    }

    public struct CollidablePairComparer : IEqualityComparerRef<CollidablePair>
    {
        //The order of collidables in the pair should not affect equality or hashing. The broad phase is not guaranteed to provide a reliable order.
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public bool Equals(ref CollidablePair a, ref CollidablePair b)
        {
            return Unsafe.As<CollidablePair, ulong>(ref a) == Unsafe.As<CollidablePair, ulong>(ref b);
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public int Hash(ref CollidablePair item)
        {
            return (int)(item.A.packed ^ item.B.packed);
        }
    }


    //Individual pair testers are designed to be used outside of the narrow phase. They need to be usable for queries and such, so all necessary data must be gathered externally.
    public struct SpherePairTester
    {
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public static void Test(
            ref Vector<float> radiiA, ref Vector<float> radiiB,
            ref Vector<float> minimumDepth,
            ref Vector3Wide relativePositionB,
            out Vector3Wide relativeContactPosition, out Vector3Wide contactNormal, out Vector<float> depth, out Vector<int> contactCount)
        {
            Vector3Wide.Length(ref relativePositionB, out var centerDistance);
            var inverseDistance = Vector<float>.One / centerDistance;
            Vector3Wide.Scale(ref relativePositionB, ref inverseDistance, out contactNormal);
            var normalIsValid = Vector.GreaterThan(centerDistance, Vector<float>.Zero);
            //Arbitrarily choose the (0,1,0) if the two spheres are in the same position. Any unit length vector is equally valid.
            contactNormal.X = Vector.ConditionalSelect(normalIsValid, contactNormal.X, Vector<float>.Zero);
            contactNormal.Y = Vector.ConditionalSelect(normalIsValid, contactNormal.Y, Vector<float>.One);
            contactNormal.Z = Vector.ConditionalSelect(normalIsValid, contactNormal.Z, Vector<float>.Zero);
            depth = radiiA + radiiB - centerDistance;
            //The position should be placed at the average of the extremePoint(a, a->b) and extremePoint(b, b->a). That puts it in the middle of the overlapping or nonoverlapping interval.
            //The contact normal acts as the direction from a to b.
            Vector3Wide.Scale(ref contactNormal, ref radiiA, out var extremeA);
            Vector3Wide.Scale(ref contactNormal, ref radiiB, out var extremeB);
            //note the following subtraction: contactNormal goes from a to b, so the negation pushes the extreme point in the proper direction from b to a.
            Vector3Wide.Subtract(ref relativePositionB, ref extremeB, out extremeB);
            Vector3Wide.Add(ref extremeA, ref extremeB, out relativeContactPosition);
            var scale = new Vector<float>(0.5f);
            Vector3Wide.Scale(ref relativeContactPosition, ref scale, out relativeContactPosition);
            contactCount = Vector.ConditionalSelect(Vector.GreaterThanOrEqual(depth, minimumDepth), Vector<int>.One, Vector<int>.Zero);
        }
    }

    public struct CollidableDataSource
    {
        public Shapes Shapes;
        public Bodies Bodies;

        public CollidableDataSource(Bodies bodies, Shapes shapes)
        {
            Shapes = shapes;
            Bodies = bodies;
        }
        //public void GatherRigidPair(ref CollidablePair pair,
        //    out int shapeIndexA, out int shapeIndexB,
        //    int laneIndex, ref Vector<float> minimumDepth, ref Vector3Wide localPositionB, out QuaternionWide orientationA, out QuaternionWide orientationB)
        //{
        //    ref var minimumDepthLane = ref GatherScatter.Get(ref minimumDepth, laneIndex);
        //    BodyPose poseA, poseB;
        //    if (pair.A.IsStatic)
        //    {
        //        //TODO: When non-body collidables exist, this needs to seek out the proper data source.
        //        poseA = new BodyPose();
        //        minimumDepthLane = 0;
        //    }
        //    else
        //    {
        //        var bodyIndex = Bodies.HandleToIndex[pair.A.CollidableIndex];
        //        Bodies.GetPoseByIndex(bodyIndex, out poseA);
        //        minimumDepthLane = -Bodies.Collidables[bodyIndex].SpeculativeMargin;
        //    }
        //    if (pair.B.IsStatic)
        //    {
        //        poseB = new BodyPose();
        //    }
        //    else
        //    {
        //        var bodyIndex = Bodies.HandleToIndex[pair.B.CollidableIndex];
        //        Bodies.GetPoseByIndex(bodyIndex, out poseB);
        //        minimumDepthLane -= Bodies.Collidables[bodyIndex].SpeculativeMargin;
        //    }
        //    BodyPose.GetRelativePosition(ref poseA, ref poseB, out var localB);
        //    GatherScatter.SetLane(ref localPositionB, laneIndex, ref localB, 3);
        //    GatherScatter.SetLane(ref orientationA.X, laneIndex, ref poseA.Orientation.X, 4);
        //    GatherScatter.SetLane(ref orientationB.X, laneIndex, ref poseB.Orientation.X, 4);
        //}
        //We special case the position-only version for the sake of sphere-sphere tests. Kinda questionable from a maintainability standpoint, but hey, super minor speedup!
        public void GatherRigidPair(ref CollidablePair pair,
            out int shapeIndexA, out int shapeIndexB,
            int laneIndex, ref Vector<float> minimumDepth, ref Vector3Wide localPositionB)
        {
            shapeIndexA = 0;
            shapeIndexB = 0;
            //ref var minimumDepthLane = ref GatherScatter.Get(ref minimumDepth, laneIndex);
            //BodyPose poseA, poseB;
            //if (pair.A.IsStatic)
            //{
            //    //TODO: When non-body collidables exist, this needs to seek out the proper data source.
            //    poseA = new BodyPose();
            //    minimumDepthLane = 0;
            //}
            //else
            //{
            //    var bodyIndex = Bodies.HandleToIndex[pair.A.CollidableIndex];
            //    Bodies.GetLane(bodyIndex, out poseA);
            //    minimumDepthLane = -Bodies.Collidables[bodyIndex].SpeculativeMargin;
            //}
            //if (pair.B.IsStatic)
            //{
            //    poseB = new BodyPose();
            //}
            //else
            //{
            //    var bodyIndex = Bodies.HandleToIndex[pair.B.CollidableIndex];
            //    Bodies.GetPoseByIndex(bodyIndex, out poseB);
            //    minimumDepthLane -= Bodies.Collidables[bodyIndex].SpeculativeMargin;
            //}
            //BodyPose.GetRelativePosition(ref poseA, ref poseB, out var localB);
            //GatherScatter.SetLane(ref localPositionB, laneIndex, ref localB, 3);
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        internal void GetShapeBuffer<TShape>(out Buffer<TShape> shapesBuffer) where TShape : struct, IShape
        {
            var untypedBatch = Shapes[TypeIds<IShape>.GetId<TShape>()];
            shapesBuffer = Unsafe.As<ShapeBatch, ShapeBatch<TShape>>(ref untypedBatch).shapes;
        }
    }

    /// <summary>
    /// Performs a collision test on a set of collidable pairs as a batch. Responsible for gathering necessary state, executing batches, and reporting results to pair owners.
    /// </summary>
    public abstract class CollidablePairTester
    {
        public int BatchSize { get; protected set; }
        public abstract void Test(ref QuickList<PairJob, Buffer<PairJob>> jobs, ref Continuations owners, ref CollidableDataSource collidableSource);
    }


    public enum ContinuationType
    {
        /// <summary>
        /// Convex-convex pair which will directly produce constraints.
        /// </summary>
        ConvexConstraintGenerator = 0,
        /// <summary>
        /// One of potentially multiple substeps produced by a collidable pair using substepped continuous collision detection.
        /// </summary>
        Substep = 1,
        /// <summary>
        /// Inner sphere test associated with a collidable pair using inner sphere continuous collision detection.
        /// </summary>
        InnerSphere = 2,
        /// <summary>
        /// Compound-convex or compound-compound subpair.
        /// </summary>
        Compound = 3,
        /// <summary>
        /// The pair belongs to a mesh-convex pair.
        /// </summary>
        Mesh = 3,
        /// <summary>
        /// Marks a pair as being owned by a mesh-mesh pair.
        /// </summary>
        MeshMesh = 4,
        /// <summary>
        /// Marks a pair as being owned by a mesh-compound pair.
        /// </summary>
        MeshCompound = 5,


    }
    public struct Continuation
    {
        uint packed;
        const int IndexBitCount = 28;
        public ContinuationType Type { [MethodImpl(MethodImplOptions.AggressiveInlining)]get { return (ContinuationType)(packed >> IndexBitCount); } }

        public int Index { [MethodImpl(MethodImplOptions.AggressiveInlining)]get { return (int)(packed & ((1 << IndexBitCount) - 1)); } }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public Continuation(ContinuationType type, int index)
        {
            packed = ((uint)type << 28) | (uint)index;
        }
    }


    public struct ConvexContactManifoldWide
    {
        public Vector3Wide OffsetA0;
        public Vector3Wide OffsetB0;
        public Vector3Wide OffsetA1;
        public Vector3Wide OffsetB1;
        public Vector3Wide OffsetA2;
        public Vector3Wide OffsetB2;
        public Vector3Wide OffsetA3;
        public Vector3Wide OffsetB3;
        /// <summary>
        /// Surface basis for the manifolds, defining both normal and tangents.
        /// </summary>
        public QuaternionWide Basis;
        /// <summary>
        /// The number of contacts in the manifolds.
        /// </summary>
        public Vector<int> Count;
        /// <summary>
        /// The maximum number of contacts that this pair type could ever generate.
        /// </summary>
        public int MaximumCount;
    }

    //TODO: If we have any pair types that compute manifolds in a non-simd batched way, you'll need an overload of the continuations executor which is able to take them.
    //This is pretty likely- going wide on hull-hull is going to be tricky, and there's a lot of opportunity for internal SIMD usage.

    public enum PairConstraintType
    {
        Convex1 = 0, Convex2 = 1, Convex3 = 2, Convex4 = 3,
        Nonconvex1 = 4, Nonconvex2 = 5, Nonconvex3 = 6, Nonconvex4 = 7
    }
    public struct PairConstraintReference
    {
        uint packed;
        public PairConstraintType Type { [MethodImpl(MethodImplOptions.AggressiveInlining)]get { return (PairConstraintType)(packed >> 29); } }
        public int Handle { [MethodImpl(MethodImplOptions.AggressiveInlining)]get { return (int)(packed & ((1 << 29) - 1)); } }
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public PairConstraintReference(PairConstraintType type, int constraintHandle)
        {
            Debug.Assert(constraintHandle < (1 << 29), "Constraint handles are assumed to be contiguous, positive, and not absurdly large.");
            packed = ((uint)type << 29) | (uint)constraintHandle;
        }
    }

    public struct Continuations
    {
        public Solver Solver;
        public NarrowPhase NarrowPhase;
        public Continuations(Solver solver, NarrowPhase narrowPhase)
        {
            Solver = solver;
            NarrowPhase = narrowPhase;
        }

        //We're basically just using a fixed size stack allocation. We could use stackalloc instead (it'll likely result in the same amount of localsinit), but shrug.
        struct CachedImpulses
        {
            //By convention, we only ever create manifolds with at most 4 contacts.
            public Vector4 PositionAndImpulse0;
            public Vector4 PositionAndImpulse1;
            public Vector4 PositionAndImpulse2;
            public Vector4 PositionAndImpulse3;
            public int Count;

        }
        /// <summary>
        /// Gathers and heuristically redistributes accumulated impulses from the previous frame's constraint data.
        /// </summary>
        /// <param name="constraintHandle">Handle to the constraint to look up.</param>
        /// <param name="impulses">Impulses and associated data required to perform accumulated impulse redistribution.</param>
        void RedistributeAccumulatedImpulses(PairConstraintReference constraint, PairConstraintType targetType)
        {
            //This establishes a hardcoded relationship between narrow phase pairs and solver types. This could cause some awkwardness, but it is a relatively simple approach
            //that avoids the need for virtual invocation when gathering and scattering accumulated impulses.
            //TODO: If greater extensibility is required later, you could consider storing data which allow the direct calculation of accumulated impulse pointers without knowing the type.
            //There's a little complexity there in that we still have to be mindful of the convex vs nonconvex constraint split, but it could be made to work. The data would likely already
            //get pulled into cache because the type batch's accumulated impulses and prestep data pointers have to be pulled in.

            //This callback is responsible for both determining if the constraint should exist at all and for filling any necessary material properties.
            //It also acts as user notification of the manifold. An event system could be built on top of this.
            if (NarrowPhase.ConstraintCallback.ShouldAllowConstraint(collidableReferenceA, collidableReferenceB, ref constraintDescription))
            {
                //So now we assume that the material properties are set and this constraint is ready to be put into the solver itself.
                if (targetType == constraint.Type)
                {
                    //We can directly change the accumulated impulses within the constraint.
                    //No new constraint needs to be created.
                    Solver.GetConstraintReference(constraint.Handle, out var reference);
                    switch (constraint.Type)
                    {
                        case PairConstraintType.Convex1:
                            break;
                        case PairConstraintType.Convex2:
                            break;
                        case PairConstraintType.Convex3:
                            break;
                        case PairConstraintType.Convex4:
                            {
                                var batch = Unsafe.As<TypeBatch, ContactManifold4TypeBatch>(ref reference.TypeBatch);
                                batch.AccumulatedImpulses[reference.IndexInTypeBatch];
                            }
                            break;
                        case PairConstraintType.Nonconvex1:
                            break;
                        case PairConstraintType.Nonconvex2:
                            break;
                        case PairConstraintType.Nonconvex3:
                            break;
                        case PairConstraintType.Nonconvex4:
                            break;
                    }
                }
                else
                {
                    //The constraint type is changing, so we need to prepare a removal for the old constraint while setting up a new constraint.
                    //This requires two parts:
                    //1) Add the new constraint. For now, we use a simple spinlock to directly add the new constraint. Adding constraints to the solver is guaranteed to not change
                    //any existing constraint index, so it's fine to do it at the same time as other threads are still working on updating accumulated impulses.
                    //Later on, we may want to look into batching adds. Even a spinlock is far from free, especially if there is any contest. If we could batch up a number of adds at a time,
                    //we may be able to reduce synchronization overhead and get better throughput. On the other hand, such a batching process would require that we store the pending 
                    //constraints, which will add cache pressure and lead to more stalls, and it could be that the spinlock is sufficiently rarely contested that it is a trivial cost.
                    //Experimentation will be required.
                    //2) Add the old constraint to a 'to remove' set. This is not just for the sake of synchronization efficiency. We can only trigger removals after all 
                    //constraint modifications are complete. Removals can change the order of constraints in a type batch, so if removals were allowed during this phase,
                    //even the case where the previous and current type are the same would require synchronization. That's really bad, since state transitions are actually relatively rare.
                    //The good news is that we can handle constraint removals in parallel in the same way that we do for deactivation, so even if there is a ton of constraint churn,
                    //multithread utilization will still be high.

                    bool taken = false;
                    NarrowPhase.AddLock.TryEnter(ref taken);
                    switch (targetType)
                    {
                        case PairConstraintType.Convex1:
                            break;
                        case PairConstraintType.Convex2:
                            break;
                        case PairConstraintType.Convex3:
                            break;
                        case PairConstraintType.Convex4:
                            {
                                ContactManifold4Constraint description;
                                description.
                            }
                            break;
                        case PairConstraintType.Nonconvex1:
                            break;
                        case PairConstraintType.Nonconvex2:
                            break;
                        case PairConstraintType.Nonconvex3:
                            break;
                        case PairConstraintType.Nonconvex4:
                            break;
                    }
                    NarrowPhase.AddLock.Exit();
                }
            }



        }

        public unsafe void Execute(Buffer<PairJob> jobs, int jobStart, int jobCount, ref ConvexContactManifoldWide manifold)
        {
            //Given the use of virtuals elsewhere, it may seem a bit odd to use a hardcoded switch here.
            //Especially because these are not guaranteed to be the only types ever supported. Some extensions may demand new entries here.
            //As usual, this comes down to performance:
            //1) Continuation types vary on a per-pair basis, rather than on a per-batch basis. This was a tradeoff:
            //the greater the degree of batch segmentation, the less work could be batched together, and the more pending pairs sit around in L1 cache.
            //Since the contact manifold calculation is the dominant cost on nontrivial pairs, the more full-occupancy SIMD dispatches we can do, the better.

            //2) Having established per-pair invocation, virtual invocations tend to be slower than contiguous switch statements by about 8-15 cycles.
            //For 100000 pairs on a 3770K-like CPU, we'd expect a virtual implementation to run 50-100us slower than a switch implementation.
            //That's pretty small, but this kind of thing adds up. 

            //(This choice is something to monitor over time in terms of virtual/switch codegen, batching, and extensibility. We already do a virtual dispatch at the 
            //batch level, so if it turns out pairType x continuationType batching is okay in practice and the hardcodedness is getting in the way, we can switch.)
            var pairBase = (PairJob*)jobs.Memory + jobStart;
            for (int i = 0; i < jobCount; ++i)
            {
                ref var job = ref *(pairBase + i);
                switch (job.Continuation.Type)
                {
                    case ContinuationType.ConvexConstraintGenerator:

                        if (PairCache.TryGetValue(ref job.Pair, out var constraintHandle))
                        {
                            //This pair is associated with a constraint. 
                            GatherAccumulatedImpulses(constraintHandle, out var previousImpulses)



                        }
                        //TODO: When we support non-constraint manifold storage targets for coldet-only use cases, we'll need to avoid attempting to make changes to the solver.
                        break;

                }
            }
        }

    }



    public struct PairJob
    {
        public CollidablePair Pair;
        public Continuation Continuation;
    }


    public class SpherePairGatherExecuteReport : CollidablePairTester
    {
        public SpherePairGatherExecuteReport()
        {
            BatchSize = 32;
        }
        //TODO: Compound children don't have a collidable reference, so they cannot be gathered in the same way.
        //Mesh children don't have a collidable reference, nor do they have a shape reference, so they can't be gathered in the same way.
        //We'll need to refactor this interface. 
        //Compound-Convex would decompose into a series of ChildConvex-Convex that know to gather the pose from the compound pose + child relative pose.
        //Mesh-Convex would decompose into a series of MeshTriangle-Convex pairs that gather vertex data from the mesh transform + local vertex positions.
        //Ideally we could find a good layout that shares as much as possible- it's not required that we have a separate batch for ChildSphere-Box, for example.
        //(However, in the mesh case, it's unlikely that we will support a standalone triangle shape, so there will likely be a MeshTriangle-Box and so on.)

        //Note that the collidable pair tester itself has no dynamic state.
        //This will be called from many threads. The caller is responsible for maintaining the necessary state in a thread safe way.
        public override void Test(ref QuickList<PairJob, Buffer<PairJob>> jobs, ref Continuations owners, ref CollidableDataSource collidableSource)
        {
            collidableSource.GetShapeBuffer(out Buffer<Sphere> shapes);
            Vector<float> radiiA, radiiB;
            Vector3Wide localPositionB;
            Vector<float> minimumDepth;
            ref var radiiStartA = ref Unsafe.As<Vector<float>, float>(ref radiiA);
            ref var radiiStartB = ref Unsafe.As<Vector<float>, float>(ref radiiB);
            for (int pairIndex = 0; pairIndex < jobs.Count; pairIndex += Vector<float>.Count)
            {
                var count = jobs.Count - pairIndex;
                if (count > Vector<float>.Count)
                    count = Vector<float>.Count;
                //Gather everything necessary for the pair.
                for (int innerIndex = 0; innerIndex < count; ++innerIndex)
                {
                    ref var pair = ref jobs[pairIndex + innerIndex];
                    //For pairs between different shapes, this gather phase will need to work out which entry needs to go first. 
                    //For example, if the sphere-box pair always uses the sphere first, then we might need to swap the order of A and B.
                    //This is a sphere-sphere pair, so there's no need.
                    collidableSource.GatherRigidPair(ref pair.Pair, out var shapeIndexA, out var shapeIndexB, innerIndex, ref minimumDepth, ref localPositionB);
                    Unsafe.Add(ref radiiStartA, innerIndex) = shapes[shapeIndexA].Radius;
                    Unsafe.Add(ref radiiStartB, innerIndex) = shapes[shapeIndexB].Radius;
                }
                //TODO: Check type punning impact on codegen. Had bugs and perf issues with that in the past.
                SpherePairTester.Test(ref radiiA, ref radiiB, ref minimumDepth, ref localPositionB, out var localContactPosition, out var contactNormal, out var depth, out var contactCount);
                //Scatter results to owners.
                for (int innerIndex = 0; innerIndex < count; ++innerIndex)
                {
                    owners.Execute(jobs[pairIndex + innerIndex].Continuation);
                }
            }
        }
    }


    /// <summary>
    /// Handles collision detection for a batch of collidable pairs together once filled or forced.
    /// </summary>
    /// <remarks>This is used by a single thread to accumulate collidable pairs over time until enough have been found to justify a wide execution.</remarks>
    public struct PairBatch<TTester> where TTester : CollidablePairTester
    {
        public QuickList<CollidablePair, Buffer<CollidablePair>> PendingPairs;
        public TTester Tester;

        public PairBatch(TTester tester, BufferPool pool) : this()
        {
            Tester = tester;
            QuickList<CollidablePair, Buffer<CollidablePair>>.Create(pool.SpecializeFor<CollidablePair>(), tester.BatchSize, out PendingPairs);
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public void Add(ref CollidablePair pair)
        {
            PendingPairs.AddUnsafely(pair);
            if (PendingPairs.Count == Tester.BatchSize)
            {
                //Tester.Test(ref PendingPairs);
            }
        }

        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        public void Flush()
        {
            //if (PendingPairs.Count > 0)
            //    Tester.Test(ref PendingPairs);

        }
    }

    public class NarrowPhase
    {
        public Bodies Bodies;
        public BufferPool Pool;
        //TODO: It is possible that some types will benefit from per-overlap data, like separating axes. For those, we should have type-dedicated overlap dictionaries.
        //The majority of type pairs, however, only require a constraint handle.
        public PairCache PairCache;
        /// <summary>
        /// Lock used by the narrow phase when adding new contact constraints in parallel.
        /// </summary>
        public SpinLock AddLock;

        public NarrowPhase(Bodies bodies, BufferPool pool, int initialOverlapCapacity = 32768)
        {
            Bodies = bodies;
            Pool = pool;
            PairCache.Create(pool.SpecializeFor<CollidablePair>(), pool.SpecializeFor<PairConstraintReference>(), pool.SpecializeFor<int>(),
                SpanHelper.GetContainingPowerOf2(initialOverlapCapacity), 3, out PairCache);

        }

        public void EnsureCapacity(int overlapCapacity)
        {
            //TODO: If there are type specialized overlap dictionaries, this must be updated.
            PairCache.EnsureCapacity(overlapCapacity, Pool.SpecializeFor<CollidablePair>(), Pool.SpecializeFor<PairConstraintReference>(), Pool.SpecializeFor<int>());

        }



        public void HandleOverlap(CollidableReference a, CollidableReference b)
        {
            if (!OverlapFilter.ShouldAllow(a, b))
                return;
            var staticness = (a.packed >> 31) | ((b.packed & 0x7FFFFFFF) >> 30);
            switch (staticness)
            {
                case 0:
                    {
                        //Both references are bodies.
                        //This is a body. In order to dispatch it properly, we need to know some metadata.
                        //TODO: Once inactive bodies exist, this will need to be updated.
                        ref var aCollidable = ref Bodies.Collidables[a.CollidableIndex];
                        ref var bCollidable = ref Bodies.Collidables[b.CollidableIndex];
                        if (aCollidable.Continuity.UseSubstepping || bCollidable.Continuity.UseSubstepping)
                        {
                            //Pull the velocity information for all involved bodies. We will request a number of steps that will cover the motion path.
                            //number of substeps = min(maximum substep count, 1 + floor(estimated displacement / step length)), where
                            //estimated displacement = dt * (length(linear velocity A - linear velocity B) +
                            //                               maximum radius A * (length(angular velocity A) + maximum radius B * length(angular velocity B)) 
                            //Once we have a number of 
                            //We use the minimum step length of each contributing collidable. Treat non-substepping collidables as having a step length of infinity.
                            var stepLengthA = aCollidable.Continuity.UseSubstepping ? aCollidable.Continuity.MaximumStepLength : float.MaxValue;
                            var stepLengthB = bCollidable.Continuity.UseSubstepping ? bCollidable.Continuity.MaximumStepLength : float.MaxValue;
                            float stepLength = stepLengthA < stepLengthB ? stepLengthA : stepLengthB;
                        }
                    }
                    break;
                case 1:
                    {
                        //Collidable a is a body, b is a static.
                        //TODO: Once non-body collidables exist, this will need to be updated.
                    }
                    break;
                case 2:
                    {
                        //Collidable a is a static, b is a body.
                        //TODO: Once non-body collidables exist, this will need to be updated.
                    }
                    break;
                case 3:
                    {
                        //Both collidables are statics. This is a bit of a weird situation- under normal conditions, static bodies will belong to the 
                        //'inactive' broad phase tree, and the inactive tree is not tested against itself. The user must have configured this static to be in the active tree to act
                        //as a detector or something along those lines.
                        //TODO: Once non-body collidables exist, this will need to be updated.
                    }
                    break;
            }

        }
    }
}